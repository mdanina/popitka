#!/usr/bin/env python3
"""
Gradio –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ç–æ—Ä–∞.
–ó–∞–ø—É—Å–∫: python ui.py
"""

import os
import gc
import json
import tempfile
import torch
import gradio as gr
from pathlib import Path
from datetime import datetime
from faster_whisper import WhisperModel

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å NeMo
try:
    from nemo.collections.asr.models import ClusteringDiarizer
    from omegaconf import OmegaConf
    NEMO_AVAILABLE = True
except ImportError:
    NEMO_AVAILABLE = False
    print("‚ö† NeMo –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")


def get_device():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ."""
    if torch.cuda.is_available():
        return "cuda"
    return "cpu"


def format_timestamp(seconds: float) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è."""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    ms = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}.{ms:03d}"


def create_nemo_manifest(audio_path: str, manifest_path: str):
    """–°–æ–∑–¥–∞—ë—Ç manifest –¥–ª—è NeMo."""
    meta = {
        "audio_filepath": audio_path,
        "offset": 0,
        "duration": None,
        "label": "infer",
        "text": "-",
        "num_speakers": None,
        "rttm_filepath": None,
        "uem_filepath": None
    }
    with open(manifest_path, 'w') as f:
        json.dump(meta, f)
        f.write('\n')


def run_nemo_diarization(audio_path: str, output_dir: str):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é NeMo."""
    manifest_path = os.path.join(output_dir, "manifest.json")
    create_nemo_manifest(audio_path, manifest_path)

    config = OmegaConf.create({
        "diarizer": {
            "manifest_filepath": manifest_path,
            "out_dir": output_dir,
            "oracle_vad": False,
            "collar": 0.25,
            "ignore_overlap": True,
            "vad": {
                "model_path": "vad_multilingual_marblenet",
                "external_vad_manifest": None,
                "parameters": {
                    "window_length_in_sec": 0.15,
                    "shift_length_in_sec": 0.01,
                    "smoothing": "median",
                    "overlap": 0.5,
                    "onset": 0.1,
                    "offset": 0.1,
                    "pad_onset": 0.1,
                    "pad_offset": 0,
                    "min_duration_on": 0.2,
                    "min_duration_off": 0.2,
                    "filter_speech_first": True
                }
            },
            "speaker_embeddings": {
                "model_path": "titanet_large",
                "parameters": {
                    "window_length_in_sec": [1.5, 1.25, 1.0, 0.75, 0.5],
                    "shift_length_in_sec": [0.75, 0.625, 0.5, 0.375, 0.25],
                    "multiscale_weights": [1, 1, 1, 1, 1],
                    "save_embeddings": False
                }
            },
            "clustering": {
                "parameters": {
                    "oracle_num_speakers": False,
                    "max_num_speakers": 8,
                    "enhanced_count_thres": 80,
                    "max_rp_threshold": 0.25,
                    "sparse_search_volume": 30,
                    "maj_vote_spk_count": False
                }
            }
        }
    })

    sd_model = ClusteringDiarizer(cfg=config)
    sd_model.diarize()

    rttm_file = os.path.join(output_dir, "pred_rttms",
                             Path(audio_path).stem + ".rttm")

    results = []
    if os.path.exists(rttm_file):
        with open(rttm_file, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 8:
                    start = float(parts[3])
                    duration = float(parts[4])
                    speaker = parts[7]
                    results.append({
                        "start": start,
                        "end": start + duration,
                        "speaker": speaker
                    })

    return results


def assign_speakers(segments, diarization):
    """–ù–∞–∑–Ω–∞—á–∞–µ—Ç —Å–ø–∏–∫–µ—Ä–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞–º."""
    for segment in segments:
        seg_mid = (segment["start"] + segment["end"]) / 2
        speaker = "SPEAKER_00"
        for diar in diarization:
            if diar["start"] <= seg_mid <= diar["end"]:
                speaker = diar["speaker"]
                break
        segment["speaker"] = speaker
    return segments


def format_output(segments, format_type, diarize):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—ã–≤–æ–¥."""
    if format_type == "txt":
        lines = []
        for seg in segments:
            start = format_timestamp(seg["start"])
            end = format_timestamp(seg["end"])
            text = seg["text"].strip()
            if diarize:
                speaker = seg.get("speaker", "Unknown")
                lines.append(f"[{start} - {end}] {speaker}: {text}")
            else:
                lines.append(f"[{start} - {end}] {text}")
        return "\n".join(lines)

    elif format_type == "srt":
        lines = []
        for i, seg in enumerate(segments, 1):
            start = format_timestamp(seg["start"]).replace('.', ',')
            end = format_timestamp(seg["end"]).replace('.', ',')
            text = seg["text"].strip()
            if diarize:
                speaker = seg.get("speaker", "")
                text = f"[{speaker}] {text}"
            lines.append(f"{i}\n{start} --> {end}\n{text}\n")
        return "\n".join(lines)

    elif format_type == "json":
        return json.dumps({"segments": segments}, ensure_ascii=False, indent=2)


def transcribe(audio_file, language, model_size, diarize, output_format, progress=gr.Progress()):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏."""

    if audio_file is None:
        return "–û—à–∏–±–∫–∞: –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª", None

    device = get_device()
    compute_type = "float16" if device == "cuda" else "int8"

    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper
        progress(0.1, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper...")

        whisper_model = WhisperModel(
            model_size,
            device=device,
            compute_type=compute_type
        )

        # 2. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
        progress(0.3, desc="–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è...")

        segments_gen, info = whisper_model.transcribe(
            audio_file,
            language=language,
            beam_size=5,
            vad_filter=True,
            vad_parameters=dict(min_silence_duration_ms=500)
        )

        segments = []
        for seg in segments_gen:
            segments.append({
                "start": seg.start,
                "end": seg.end,
                "text": seg.text
            })

        # –û—á–∏—Å—Ç–∫–∞
        del whisper_model
        gc.collect()
        if device == "cuda":
            torch.cuda.empty_cache()

        # 3. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è
        if diarize and NEMO_AVAILABLE:
            progress(0.6, desc="–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤...")

            with tempfile.TemporaryDirectory() as temp_dir:
                diarization = run_nemo_diarization(audio_file, temp_dir)
                segments = assign_speakers(segments, diarization)

            gc.collect()
            if device == "cuda":
                torch.cuda.empty_cache()

        progress(0.9, desc="–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")

        # 4. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        result_text = format_output(segments, output_format, diarize and NEMO_AVAILABLE)

        # 5. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        ext = {"txt": ".txt", "srt": ".srt", "json": ".json"}[output_format]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"transcript_{timestamp}{ext}"

        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix=ext, delete=False, encoding='utf-8')
        temp_file.write(result_text)
        temp_file.close()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        num_segments = len(segments)
        duration = segments[-1]["end"] if segments else 0
        num_speakers = len(set(s.get("speaker", "") for s in segments)) if diarize else 0

        stats = f"–°–µ–≥–º–µ–Ω—Ç–æ–≤: {num_segments} | –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {format_timestamp(duration)}"
        if diarize and NEMO_AVAILABLE:
            stats += f" | –°–ø–∏–∫–µ—Ä–æ–≤: {num_speakers}"

        progress(1.0, desc="–ì–æ—Ç–æ–≤–æ!")

        return result_text, temp_file.name, stats

    except Exception as e:
        return f"–û—à–∏–±–∫–∞: {str(e)}", None, ""


# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
def create_ui():
    device = get_device()
    device_info = f"üñ• –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}"
    if device == "cuda":
        device_info += f" ({torch.cuda.get_device_name(0)})"

    with gr.Blocks(title="–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ç–æ—Ä", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# üéô –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ç–æ—Ä –∞—É–¥–∏–æ")
        gr.Markdown("Whisper + NeMo | –ë–µ–∑ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∏ —Ç–æ–∫–µ–Ω–æ–≤")
        gr.Markdown(f"**{device_info}**")

        with gr.Row():
            with gr.Column(scale=1):
                audio_input = gr.Audio(
                    label="–ê—É–¥–∏–æ—Ñ–∞–π–ª",
                    type="filepath",
                    sources=["upload", "microphone"]
                )

                language = gr.Dropdown(
                    choices=[("–†—É—Å—Å–∫–∏–π", "ru"), ("English", "en")],
                    value="ru",
                    label="–Ø–∑—ã–∫"
                )

                model_size = gr.Dropdown(
                    choices=["tiny", "base", "small", "medium", "large-v2", "large-v3"],
                    value="small",
                    label="–ú–æ–¥–µ–ª—å Whisper",
                    info="small —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è 6GB VRAM"
                )

                diarize = gr.Checkbox(
                    value=NEMO_AVAILABLE,
                    label="–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤",
                    interactive=NEMO_AVAILABLE,
                    info="–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç" if NEMO_AVAILABLE else "NeMo –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
                )

                output_format = gr.Radio(
                    choices=[("–¢–µ–∫—Å—Ç", "txt"), ("–°—É–±—Ç–∏—Ç—Ä—ã SRT", "srt"), ("JSON", "json")],
                    value="txt",
                    label="–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞"
                )

                transcribe_btn = gr.Button("üöÄ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å", variant="primary", size="lg")

            with gr.Column(scale=2):
                output_text = gr.Textbox(
                    label="–†–µ–∑—É–ª—å—Ç–∞—Ç",
                    lines=20,
                    max_lines=30,
                    show_copy_button=True
                )

                with gr.Row():
                    stats_text = gr.Textbox(label="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", lines=1, interactive=False)
                    download_file = gr.File(label="–°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª")

        # –û–±—Ä–∞–±–æ—Ç–∫–∞
        transcribe_btn.click(
            fn=transcribe,
            inputs=[audio_input, language, model_size, diarize, output_format],
            outputs=[output_text, download_file, stats_text]
        )

        # –ü—Ä–∏–º–µ—Ä—ã
        gr.Markdown("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
        gr.Markdown("""
        - **–î–ª—è RTX 4050 (6GB):** –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å `small` –∏–ª–∏ `medium`
        - **–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:** ~10-15 –º–∏–Ω—É—Ç –Ω–∞ 1 —á–∞—Å –∞—É–¥–∏–æ
        - **–§–æ—Ä–º–∞—Ç—ã:** MP3, WAV, M4A, FLAC –∏ –¥—Ä—É–≥–∏–µ
        """)

    return demo


if __name__ == "__main__":
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        inbrowser=True
    )
